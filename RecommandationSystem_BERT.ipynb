{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test 010424\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YApUaxTESPZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMREQHSvERa5"
      },
      "outputs": [],
      "source": [
        "def read_data(file_path):\n",
        "    \"\"\"\n",
        "       params:\n",
        "           file_path: 文件路径\n",
        "       return:\n",
        "           data: 读取的数据列表，每行一条样本\n",
        "\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            # 过长的评论文本会超出io限制报错。 暂时忽略\n",
        "            try:\n",
        "                # 字符串预处理\n",
        "                # 布尔替换成python的习惯\n",
        "                str_text = line.replace(\"true\", \"True\")\n",
        "                str_text = str_text.replace(\"false\", \"False\")\n",
        "                # 转成字典形式\n",
        "                raw_sample = eval(str_text)\n",
        "                data.append([raw_sample['reviewerID'],\n",
        "                             raw_sample['asin'],\n",
        "                             raw_sample['overall'],\n",
        "                             raw_sample['reviewText']])\n",
        "            except:\n",
        "                pass\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMhVma8dCuaG"
      },
      "outputs": [],
      "source": [
        "data = read_data(\"./data/raw/Beauty_5.json\")\n",
        "data_df = pd.DataFrame(data)\n",
        "data_df.columns = ['reviewerID', 'asin', 'overall', 'reviewText']\n",
        "data = data_df[\"reviewText\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEeyfqnAEkPP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "model_name='bert-base-uncased'\n",
        "# Khởi tạo tokenizer và model BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "def extract_bert_features(text, model, tokenizer, output_dim=10):\n",
        "\n",
        "\n",
        "    # Tokenize văn bản và thêm token '[CLS]' và '[SEP]'\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Feed-forward văn bản qua model BERT để trích xuất đặc trưng\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Lấy đặc trưng từ lớp embedding (lớp cuối cùng trước lớp softmax)\n",
        "    features = outputs.last_hidden_state\n",
        "\n",
        "    # Pooling các đặc trưng từ các token thành một vector đặc trưng (ở đây ta sử dụng mean pooling)\n",
        "    pooled_features = torch.mean(features, dim=1)  # Kích thước: (1, hidden_size)\n",
        "\n",
        "    # Giảm chiều feature đầu ra bằng cách sử dụng một lớp Linear\n",
        "    linear_layer = torch.nn.Linear(pooled_features.shape[1], output_dim)\n",
        "    reduced_features = linear_layer(pooled_features)\n",
        "\n",
        "    return reduced_features\n",
        "\n",
        "# Tính cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return F.cosine_similarity(vec1, vec2).item()\n",
        "\n",
        "# Tính Euclidean distance\n",
        "def euclidean_distance(vec1, vec2):\n",
        "    return torch.dist(vec1, vec2).item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAjwGUVKMDJn",
        "outputId": "daf62554-ff27-48a0-de39-4510b5cde59d"
      },
      "outputs": [],
      "source": [
        "# # Sử dụng hàm để trích xuất đặc trưng từ văn bản\n",
        "# text = \"Hello\"\n",
        "# features = extract_bert_features(text, model, tokenizer)\n",
        "# print(features)\n",
        "\n",
        "# # Sử dụng hàm để trích xuất đặc trưng từ văn bản\n",
        "# text = \"Hello\"\n",
        "# features = extract_bert_features(text, model, tokenizer)\n",
        "# print(features)\n",
        "\n",
        "# Ví dụ về sử dụng\n",
        "text1 = \"Hello, how are you?\"\n",
        "text2 = \"Hi, how are you doing?\"\n",
        "features1 = extract_bert_features(text1, model, tokenizer)\n",
        "features2 = extract_bert_features(text2, model, tokenizer)\n",
        "\n",
        "\n",
        "print(\"Cosine similarity:\", cosine_similarity(features1, features2))\n",
        "print(\"Euclidean distance:\", euclidean_distance(features1, features2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8DBaoChMSgo",
        "outputId": "430fe243-0ab7-409d-d0ae-579bafd746e4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Tạo dataset đơn giản\n",
        "train_data = [\n",
        "    {'text': 'This is a positive sentence.', 'label': 1},\n",
        "    {'text': 'I love using BERT for natural language processing tasks.', 'label': 1},\n",
        "    {'text': 'Negative sentiment is not good for the mood.', 'label': 0},\n",
        "    {'text': 'The quick brown fox jumps over the lazy dog.', 'label': 1},\n",
        "    {'text': 'I am not feeling well today.', 'label': 0}\n",
        "]\n",
        "\n",
        "# Chuẩn bị dữ liệu huấn luyện\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]['text']\n",
        "        label = self.data[idx]['label']\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': torch.tensor(label)\n",
        "        }\n",
        "\n",
        "# Chuẩn bị mô hình BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Fine-tuning mô hình\n",
        "train_dataset = CustomDataset(train_data, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\")\n",
        "\n",
        "# Lưu mô hình sau khi fine-tune\n",
        "model.save_pretrained(\"fine_tuned_bert_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "cAfziZuhRIWC",
        "outputId": "ea0cbc6e-c1eb-442d-cee0-d7d8f24c4d99"
      },
      "outputs": [],
      "source": [
        "# Load mô hình và tokenizer đã fine-tune\n",
        "model = BertForSequenceClassification.from_pretrained(\"fine_tuned_bert_model\")\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Chuẩn bị câu văn bản để test\n",
        "text = \"This is a positive sentence.\"\n",
        "\n",
        "# Tokenize câu văn bản\n",
        "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Phân loại hoặc trích xuất đặc trưng từ câu văn bản\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Nếu muốn phân loại, sử dụng logits\n",
        "logits = outputs.logits\n",
        "predicted_class = torch.argmax(logits, dim=1).item()\n",
        "print(\"Predicted class:\", predicted_class)\n",
        "\n",
        "# Nếu muốn trích xuất đặc trưng, sử dụng đặc trưng từ lớp embedding (lớp cuối cùng trước lớp softmax)\n",
        "features = outputs.last_hidden_state.mean(dim=1).squeeze(0)  # Sửa đoạn này\n",
        "print(\"Extracted features:\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMGYlfWZt5b6",
        "outputId": "036535d4-e980-4e6c-cfdb-42b1bf47c9d7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = read_data(\"/content/drive/MyDrive/Recommandation System/AMAZON_FASHION_5.json\")\n",
        "data_df = pd.DataFrame(data)\n",
        "data_df.columns = ['reviewerID', 'asin', 'overall', 'reviewText']\n",
        "data = data_df[\"reviewText\"].tolist()\n",
        "# In kết quả xuống hàng mỗi record\n",
        "for record in data[:20]:  # Chỉ in 20 record đầu tiên\n",
        "    print(record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvoDDSBEs7h7",
        "outputId": "7c435f6a-5ba9-484c-8762-b3b1afbaa3db"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet, stopwords\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Loại bỏ ký tự không cần thiết và chuyển đổi thành chữ thường\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def get_common_words(texts, n):\n",
        "    # Tiền xử lý văn bản và loại bỏ từ dừng\n",
        "    preprocessed_texts = [remove_stopwords(preprocess_text(text)) for text in texts]\n",
        "\n",
        "    # Tách thành các từ riêng lẻ\n",
        "    words = ' '.join(preprocessed_texts).split()\n",
        "\n",
        "    # Đếm tần suất của các từ\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Chọn các từ có tần suất xuất hiện cao nhất\n",
        "    common_words = [word for word, count in word_counts.most_common(n)]\n",
        "\n",
        "    return common_words\n",
        "\n",
        "def convert_records_to_dataset(records):\n",
        "    dataset_words = []\n",
        "    for record in records:\n",
        "        # Tiền xử lý văn bản\n",
        "        preprocessed_text = preprocess_text(record)\n",
        "\n",
        "        # Loại bỏ các từ dừng\n",
        "        filtered_words = remove_stopwords(preprocessed_text)\n",
        "\n",
        "        # Thêm các từ vào dataset_words\n",
        "        words = filtered_words.split()\n",
        "        for word in words:\n",
        "          dataset_words.append(word)\n",
        "\n",
        "    return dataset_words\n",
        "\n",
        "def semantic_similarity(word1, word2):\n",
        "    # Tìm các synset (nhóm đồng nghĩa) của từ 1 và từ 2\n",
        "    synsets1 = wordnet.synsets(word1)\n",
        "    synsets2 = wordnet.synsets(word2)\n",
        "    print(synsets1)\n",
        "    print(synsets2)\n",
        "    max_similarity = 0\n",
        "\n",
        "    # Lặp qua các synset của từ 1 và từ 2 để tìm độ tương đồng cao nhất\n",
        "    for synset1 in synsets1:\n",
        "        for synset2 in synsets2:\n",
        "            similarity = synset1.wup_similarity(synset2)  # Sử dụng phương pháp Wu-Palmer để tính độ tương đồng\n",
        "            if similarity is not None and similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "\n",
        "    return max_similarity\n",
        "print(semantic_similarity(\"great\", \"best\"))\n",
        "# Ví dụ dữ liệu văn bản\n",
        "dataset_words = convert_records_to_dataset(data[:20])\n",
        "print(dataset_words)\n",
        "\n",
        "# Lấy 10 từ xuất hiện nhiều nhất trong tập dữ liệu\n",
        "common_words = get_common_words(data[:20], 10)\n",
        "\n",
        "# Tìm các từ đồng nghĩa với từng từ phổ biến\n",
        "synonyms_dict = {}\n",
        "for word in common_words:\n",
        "    synonyms = find_synonyms_in_dataset(word, dataset_words)\n",
        "    synonyms_dict[word] = synonyms\n",
        "\n",
        "# In kết quả\n",
        "for word, synonyms in synonyms_dict.items():\n",
        "    print(f\"Synonyms of '{word}': {', '.join(synonyms)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "m9_PUywf1qth",
        "outputId": "416abbf4-53dc-4afa-bd25-653726115981"
      },
      "outputs": [],
      "source": [
        "!pip install pytesseract\n",
        "!pip install pillow==10.0.0\n",
        "!pip install pillow-heif\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzbJkk0i-DGV",
        "outputId": "3674419a-fe77-4547-9c24-5a7f98f92884"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "\n",
        "# Tải xuống tập tin dữ liệu ngôn ngữ cho tiếng Việt\n",
        "!wget https://github.com/tesseract-ocr/tessdata/raw/main/vie.traineddata -P /usr/share/tesseract-ocr/4.00/tessdata/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwY_O330-VMX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Thiết lập biến môi trường TESSDATA_PREFIX\n",
        "os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/4.00/tessdata/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad55bDhZBm0C",
        "outputId": "5ebbc0dd-0b8b-4242-818f-4236b36a7755"
      },
      "outputs": [],
      "source": [
        "!pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "_oCJGBfMRKIg",
        "outputId": "e6e9f0f5-a78d-4df8-9a24-537a97c789b6"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pillow_heif import register_heif_opener\n",
        "import easyocr\n",
        "\n",
        "\n",
        "register_heif_opener()\n",
        "def ocr_vietnamese_text(image_path):\n",
        "    # Mở ảnh sử dụng Pillow\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Chuyển đổi ảnh sang định dạng PNG (nếu không phải là PNG)\n",
        "    if img.format != \"PNG\":\n",
        "        img = img.convert(\"RGB\")  # Chuyển đổi sang ảnh RGB (định dạng phổ biến nhất)\n",
        "        png_image_path = \"converted_image.png\"\n",
        "        img.save(png_image_path, format=\"PNG\")  # Lưu ảnh dưới dạng PNG\n",
        "\n",
        "    else:\n",
        "        png_image_path = image_path  # Sử dụng ảnh gốc nếu đã là định dạng PNG\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    # Sử dụng Tesseract OCR để chuyển đổi ảnh thành văn bản\n",
        "    text = pytesseract.image_to_string(png_image_path, lang='vie')\n",
        "\n",
        "    return text\n",
        "\n",
        "def ocr_vietnamese_and_english(image_path):\n",
        "    # Đường dẫn đến ảnh\n",
        "    reader = easyocr.Reader(['en', 'vi'])  # Chỉ định ngôn ngữ cần nhận dạng\n",
        "    img = Image.open(image_path)\n",
        "    # Chuyển đổi ảnh sang định dạng PNG (nếu không phải là PNG)\n",
        "    if img.format != \"PNG\":\n",
        "        img = img.convert(\"RGB\")  # Chuyển đổi sang ảnh RGB (định dạng phổ biến nhất)\n",
        "        png_image_path = \"converted_image.png\"\n",
        "        img.save(png_image_path, format=\"PNG\")  # Lưu ảnh dưới dạng PNG\n",
        "\n",
        "    else:\n",
        "        png_image_path = image_path  # Sử dụng ảnh gốc nếu đã là định dạng PNG\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    # Nhận dạng văn bản từ ảnh\n",
        "    result = reader.readtext(png_image_path)\n",
        "\n",
        "    # Tạo một đoạn văn bản từ kết quả nhận dạng\n",
        "    text = '\\n'.join([entry[1] for entry in result])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Đường dẫn đến ảnh cần chuyển đổi\n",
        "image_path = \"/content/drive/MyDrive/Recommandation System/IMG_9218.heic\"\n",
        "\n",
        "# Sử dụng hàm ocr_vietnamese_text để chuyển đổi ảnh sang văn bản\n",
        "# text = ocr_vietnamese_text(image_path)\n",
        "text = ocr_vietnamese_and_english(image_path)\n",
        "\n",
        "# In văn bản được chuyển đổi\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1lWX8OY1iKf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJmy8j-Q9jm7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Z97wr29mjb"
      },
      "source": [
        "# **Test 080424**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IVax7Vjq9xBr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jq09-vnR9q8E"
      },
      "outputs": [],
      "source": [
        "def read_data(file_path):\n",
        "    \"\"\"\n",
        "       params:\n",
        "           file_path: 文件路径\n",
        "       return:\n",
        "           data: 读取的数据列表，每行一条样本\n",
        "\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            # 过长的评论文本会超出io限制报错。 暂时忽略\n",
        "            try:\n",
        "                # 字符串预处理\n",
        "                # 布尔替换成python的习惯\n",
        "                str_text = line.replace(\"true\", \"True\")\n",
        "                str_text = str_text.replace(\"false\", \"False\")\n",
        "                # 转成字典形式\n",
        "                raw_sample = eval(str_text)\n",
        "                data.append(raw_sample)\n",
        "            except:\n",
        "                pass\n",
        "    return data\n",
        "\n",
        "def remove_duplicate_rows(df):\n",
        "    # Loại bỏ các hàng trùng lặp dựa trên cột 'reviewText'\n",
        "    df = df.drop_duplicates(subset=['reviewText', 'asin', 'reviewerID'])\n",
        "    return df\n",
        "\n",
        "def remove_duplicate_rows_ver2(df):\n",
        "    # Loại bỏ các hàng trùng lặp dựa trên cột 'reviewText'\n",
        "    df = df.drop_duplicates(subset=['reviewText', 'itemID', 'reviewerID'])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J5SZuWLY9zhy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   overall vote  verified   reviewTime      reviewerID        asin  \\\n",
            "0      5.0    3      True   06 3, 2013  A2TYZ821XXK2YZ  3426958910   \n",
            "1      5.0  NaN      True  10 11, 2014  A3OFSREZADFUDY  3426958910   \n",
            "2      5.0  NaN      True  02 11, 2014  A2VAMODP8M77NG  3426958910   \n",
            "3      4.0    3     False   12 7, 2013   AAKSLZ9IDTEH0  3426958910   \n",
            "4      5.0  NaN      True  06 12, 2016  A3OH43OZJLKI09  5557706259   \n",
            "\n",
            "                      style       reviewerName  \\\n",
            "0  {'Format:': ' Audio CD'}            Garrett   \n",
            "1  {'Format:': ' Audio CD'}                 Ad   \n",
            "2  {'Format:': ' Audio CD'}             JTGabq   \n",
            "3  {'Format:': ' Audio CD'}  john F&#039;n doe   \n",
            "4  {'Format:': ' Audio CD'}  melinda a goodman   \n",
            "\n",
            "                                          reviewText  \\\n",
            "0  This is awesome to listen to, A must-have for ...   \n",
            "1                                               bien   \n",
            "2  It was great to hear the old stuff again and I...   \n",
            "3  well best of's are a bit poison normally but t...   \n",
            "4  What can I say? This is Casting Crowns!!!This ...   \n",
            "\n",
            "                                             summary  unixReviewTime image  \n",
            "0                                      Slayer Rules!      1370217600   NaN  \n",
            "1                                         Five Stars      1412985600   NaN  \n",
            "2                        SLAYER!!!!!!!!!!!!!!!!!!!!!      1392076800   NaN  \n",
            "3  slayer greatest hits! you mean everything righ...      1386374400   NaN  \n",
            "4                    This is a good, blessing filled      1465689600   NaN  \n"
          ]
        }
      ],
      "source": [
        "data = read_data(\"/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/data/raw/Digital_Music_5.json\")\n",
        "data_df = pd.DataFrame(data)\n",
        "print(data_df.head())\n",
        "# data_df.columns = ['reviewerID', 'asin', 'overall', 'reviewText']\n",
        "\n",
        "data_rating = data_df[\"overall\"].tolist()\n",
        "data_review = data_df[\"reviewText\"].tolist()\n",
        "data_reviewerID = data_df[\"reviewerID\"].tolist()\n",
        "data_itemID = data_df[\"asin\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uzR4bKv96IP",
        "outputId": "49d8bb84-1308-42d5-f7f4-d4b3332d2b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "169781\n",
            "169781\n"
          ]
        }
      ],
      "source": [
        "print(len(data_rating))\n",
        "print(len(data_reviewerID))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh3aa--7-Omb",
        "outputId": "6ad14e4f-aef7-4a8d-9c2f-242751e6a819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            id  T_v\n",
            "0            0    1\n",
            "1            1    1\n",
            "2            2    1\n",
            "3            3    1\n",
            "4            4    1\n",
            "...        ...  ...\n",
            "169776  169776    1\n",
            "169777  169777    1\n",
            "169778  169778    1\n",
            "169779  169779    1\n",
            "169780  169780    1\n",
            "\n",
            "[169781 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# step01:\n",
        "def defineRating(data_rating):\n",
        "  T_v = []\n",
        "  for rating in data_rating:\n",
        "    if rating >= 4:\n",
        "      T_v.append(1)\n",
        "    else:\n",
        "      T_v.append(-1)\n",
        "  return T_v\n",
        "\n",
        "T_v = defineRating(data_rating)\n",
        "\n",
        "# Define the data for the dataframe\n",
        "data = {'id': range(len(T_v)), 'T_v': T_v}\n",
        "\n",
        "# Create the dataframe\n",
        "df_rating = pd.DataFrame(data)\n",
        "\n",
        "# Print the dataframe\n",
        "print(df_rating)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            id  T_v      reviewerID        asin  overall  \\\n",
            "0            0    1  A2TYZ821XXK2YZ  3426958910      5.0   \n",
            "1            1    1  A3OFSREZADFUDY  3426958910      5.0   \n",
            "2            2    1  A2VAMODP8M77NG  3426958910      5.0   \n",
            "3            3    1   AAKSLZ9IDTEH0  3426958910      4.0   \n",
            "4            4    1  A3OH43OZJLKI09  5557706259      5.0   \n",
            "...        ...  ...             ...         ...      ...   \n",
            "169776  169776    1  A1SR2T84IXOMAQ  B01HJ91MTW      5.0   \n",
            "169777  169777    1  A2SR3DWJR1PYR6  B01HJ91MTW      5.0   \n",
            "169778  169778    1  A24V7X30NIMOIY  B01HJ91MTW      5.0   \n",
            "169779  169779    1  A1LW10GYP2EYM1  B01HJ91MTW      5.0   \n",
            "169780  169780    1  A26TH3YW0QWPI7  B01HJ91MTW      5.0   \n",
            "\n",
            "                                               reviewText  \n",
            "0       This is awesome to listen to, A must-have for ...  \n",
            "1                                                    bien  \n",
            "2       It was great to hear the old stuff again and I...  \n",
            "3       well best of's are a bit poison normally but t...  \n",
            "4       What can I say? This is Casting Crowns!!!This ...  \n",
            "...                                                   ...  \n",
            "169776           Casting Crowns songs are all wonderful!!  \n",
            "169777         Just reminds you that you are never alone.  \n",
            "169778                        Good product, good service.  \n",
            "169779  I love every single song this group sings. The...  \n",
            "169780                                         Great song  \n",
            "\n",
            "[169781 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "# Thêm vào dataframe df_rating cột C_v\n",
        "df_rating['reviewerID'] = data_reviewerID\n",
        "df_rating['asin'] = data_itemID\n",
        "df_rating['overall'] = data_rating\n",
        "df_rating['reviewText'] = data_review\n",
        "# Loại bỏ các dòng có reviewText trùng lặp\n",
        "# df_rating = remove_duplicate_rows(df_rating)\n",
        "print(df_rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                min  max             list\n",
            "reviewerID                               \n",
            "A11QGZ39A7ZF0X  5.0  5.0  [5.0, 5.0, 5.0]\n",
            "A24HQ2N7332W7W  5.0  5.0            [5.0]\n",
            "A24W4W9E62FZP2  2.0  2.0            [2.0]\n",
            "A2G90R2ZU6KU5D  5.0  5.0            [5.0]\n",
            "A2UEO5XR3598GI  5.0  5.0            [5.0]\n",
            "A3CIUOJXQ5VDQ2  5.0  5.0            [5.0]\n",
            "A3H7T87S984REU  5.0  5.0            [5.0]\n",
            "A3J034YH7UG4KT  1.0  1.0            [1.0]\n",
            "A3SFRT223XXWF7  5.0  5.0            [5.0]\n",
            "A7ID5H7FWLJHC   1.0  1.0            [1.0]\n",
            "AYKOSAJTP5AVS   1.0  1.0            [1.0]\n",
            "                min  max   list\n",
            "reviewerID                     \n",
            "A24W4W9E62FZP2  2.0  2.0  [2.0]\n",
            "A3J034YH7UG4KT  1.0  1.0  [1.0]\n",
            "A7ID5H7FWLJHC   1.0  1.0  [1.0]\n",
            "AYKOSAJTP5AVS   1.0  1.0  [1.0]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ratings_grouped = df_rating.groupby('reviewerID')['overall'].agg(['min', 'max', list])\n",
        "# print(ratings_grouped)\n",
        "# filtered_users_max = ratings_grouped[ratings_grouped['max'] != 5.0]\n",
        "# print(filtered_users_max)\n",
        "\n",
        "# user_ratings = df_rating[df_rating['reviewerID'] == 'A2I9O5E0Q731GN']['overall'].tolist()\n",
        "# print(user_ratings)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SODCM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R7ku2ZiITNP",
        "outputId": "8d6a9a7b-c436-4a4c-a25b-0cf757d9f0b1"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "# Define function to extract a string's sentiment\n",
        "find_sentiment = lambda text: TextBlob(text).sentiment[0]\n",
        "# Create new column in dataframe\n",
        "C_v = data_df[\"reviewText\"].apply(find_sentiment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thêm vào dataframe df_rating cột C_v\n",
        "df_rating['C_v'] = C_v\n",
        "print(df_rating[df_rating['id'] == 98])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTuy8T9T_QYY",
        "outputId": "d5c4eeca-b4c4-4c14-e3e5-7afb2bc1bfc2"
      },
      "outputs": [],
      "source": [
        "# step02:\n",
        "import math\n",
        "\n",
        "\n",
        "def euclidean_distance(p1, p2):\n",
        "  distance = math.sqrt((p2 - p1)**2)\n",
        "  return distance\n",
        "V_d = []\n",
        "for ti, ci in zip(T_v, C_v):\n",
        "  V_d.append(euclidean_distance(ti, ci))\n",
        "  # V_d.append((ti + ci)/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thêm vào dataframe df_rating cột V_d\n",
        "df_rating['V_d'] = V_d\n",
        "print(df_rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iadewjxXDZzQ",
        "outputId": "cf12245a-0fd0-4d1c-c517-3a48ef741ba6"
      },
      "outputs": [],
      "source": [
        "# step03:\n",
        "S_pos = []\n",
        "S_neg = []\n",
        "\n",
        "i = 0\n",
        "for r in data_rating:\n",
        "  if r >= 4:\n",
        "    S_pos.append([i, r, V_d[i]])\n",
        "  else:\n",
        "    S_neg.append([i, r, V_d[i]])\n",
        "  i = i + 1\n",
        "\n",
        "print(len(S_pos))\n",
        "print(len(S_neg))\n",
        "element = None\n",
        "i = 0\n",
        "for item in S_pos:\n",
        "    if item[0] == 25:\n",
        "        element = item\n",
        "        break\n",
        "    i = i + 1\n",
        "# for item in S_neg:\n",
        "#     if item[0] == 25:\n",
        "#         element = item\n",
        "#         break\n",
        "print(S_pos[17])\n",
        "print(element)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp_RqHq3u5ir",
        "outputId": "a0ee9d35-3630-42c7-cc67-f43099476dc0"
      },
      "outputs": [],
      "source": [
        "#step04:\n",
        "\n",
        "def calculation_IQR(S, V_d, flag='pos'):\n",
        "  # Sắp xếp dữ liệu\n",
        "  sorted_data = sorted(V_d)\n",
        "\n",
        "  # Tính Q1 và Q3\n",
        "  q1_index = int(len(sorted_data) * 0.25)\n",
        "  q3_index = int(len(sorted_data) * 0.75)\n",
        "  Q1 = sorted_data[q1_index]\n",
        "  Q3 = sorted_data[q3_index]\n",
        "\n",
        "  # Tính IQR\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  # Tính F_L và F_U\n",
        "  F_L = Q1 - 1.5 * IQR\n",
        "  F_U = Q3 + 1.5 * IQR\n",
        "\n",
        "  if flag == 'pos':\n",
        "    if F_L < 0:\n",
        "      O_s =  Q3 + IQR\n",
        "    else:\n",
        "      O_s = F_U - IQR*F_L\n",
        "  else:\n",
        "    if F_U > 2:\n",
        "      O_s = Q1 - IQR\n",
        "    else:\n",
        "      O_s = Q3 - IQR*F_U\n",
        "\n",
        "  return O_s\n",
        "\n",
        "O_Spos = calculation_IQR(S_pos, V_d, flag = 'pos')\n",
        "O_Sneg = calculation_IQR(S_neg, V_d, flag = 'neg')\n",
        "\n",
        "print(O_Spos)\n",
        "print(O_Sneg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94ni1MgCzbSL",
        "outputId": "fdaa91d2-8b4c-4e4c-9979-80562da57021"
      },
      "outputs": [],
      "source": [
        "#step05:\n",
        "\n",
        "\n",
        "# S_pos: danh sách các chỉ số của các phần tử thuộc S+\n",
        "# S_neg: danh sách các chỉ số của các phần tử thuộc S-\n",
        "# VD: danh sách các giá trị của biến VD\n",
        "# OS_pos: giá trị OS+ như trong mã script\n",
        "# OS_neg: giá trị OS- như trong mã script\n",
        "\n",
        "# Khởi tạo danh sách O+ và O-\n",
        "O_pos = [0] * len(S_pos)\n",
        "O_neg = [0] * len(S_neg)\n",
        "\n",
        "# print(S_pos)\n",
        "# print(S_neg)\n",
        "count = 0\n",
        "# Điều chỉnh danh sách O+ và O- theo điều kiện trong mã script\n",
        "for i, ri in enumerate(S_pos):\n",
        "  print(i, ri)\n",
        "  if ri[2] >= O_Spos:\n",
        "    O_pos[i] = 'yes'\n",
        "    count += 1\n",
        "  else:\n",
        "    O_pos[i] = 'no'\n",
        "\n",
        "for i, ri in enumerate(S_neg):\n",
        "    if ri[2] <= O_Sneg:\n",
        "      O_neg[i] = 'yes'\n",
        "      count += 1\n",
        "    else:\n",
        "      O_neg[i] = 'no'\n",
        "\n",
        "print(count)\n",
        "# In ra kết quả\n",
        "# print(\"O+:\", O_pos)\n",
        "# print(\"O-:\", O_neg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(S_pos, columns=['id', 'rating', 'V_d'])\n",
        "df['O_pos'] = O_pos\n",
        "#In ra các phần tử có giá trị 'yes'\n",
        "print(df[df['O_pos'] == 'yes'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9myHksA3JXU",
        "outputId": "bbff483f-536f-49dc-ec37-fcde4d7cb78d"
      },
      "outputs": [],
      "source": [
        "#step06:\n",
        "# Hàm toggle để đảo ngược giá trị của một biến\n",
        "def toggle(value):\n",
        "  if value == -1:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1\n",
        "\n",
        "# Khởi tạo danh sách TV với giá trị False cho mỗi phần tử\n",
        "TV_new = T_v.copy()\n",
        "\n",
        "count_1 = 0\n",
        "for i, ri in enumerate(S_pos):\n",
        "    if O_pos[i] == 'yes':\n",
        "      count_1 += 1\n",
        "      TV_new[ri[0]] = toggle(T_v[ri[0]])\n",
        "\n",
        "for i, ri in enumerate(S_neg):\n",
        "    if O_neg[i] == 'no':\n",
        "      count_1 += 1\n",
        "      TV_new[ri[0]] = toggle(T_v[ri[0]])\n",
        "\n",
        "# print(TV_new)\n",
        "# Tạo danh sách D* bằng cách nối S+ và S-\n",
        "print(count_1)\n",
        "# D_star = S_pos + S_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thêm vào dataframe df_rating cột TV_new\n",
        "df_rating['TV_new'] = TV_new\n",
        "print(df_rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tạo DataFrame mới để lưu các hàng thỏa mãn điều kiện\n",
        "outliner = pd.DataFrame(columns=df_rating.columns)\n",
        "\n",
        "# Lặp qua các hàng của DataFrame df_rating\n",
        "for index, row in df_rating.iterrows():\n",
        "    if row['TV_new'] != row['T_v']:\n",
        "        # Thêm hàng vào DataFrame outliner\n",
        "        outliner.loc[len(outliner)] = row\n",
        "\n",
        "# In ra DataFrame outliner\n",
        "print(outliner)\n",
        "\n",
        "# Lưu DataFrame outliner vào file CSV\n",
        "outliner.to_csv('/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/outliner.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKYf8vHR7d2B",
        "outputId": "d2bc79bc-eb06-41ca-e6be-b6427ee99c92"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer for Amazon reviews\n",
        "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'  # Example model, you can choose other pre-trained models\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define the text to analyze\n",
        "text = \"I love the product\"\n",
        "def sentiment_analysis(text, model, tokenizer):\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "    # Perform sentiment analysis\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the predicted probabilities for each class\n",
        "    probs = softmax(outputs.logits, dim=1).squeeze()\n",
        "\n",
        "    # Convert the probabilities to human-readable labels\n",
        "    labels = [1, 2, 3, 4, 5]  # For this model, it predicts star ratings\n",
        "    predicted_label = labels[torch.argmax(probs).item()]\n",
        "    # print(predicted_label)\n",
        "    return predicted_label\n",
        "\n",
        "# Print the result\n",
        "print(\"Predicted sentiment:\", sentiment_analysis(text, model, tokenizer))\n",
        "print(\"Predicted sentiment:\", torch.sigmoid(outputs.logits).squeeze()[1].item())\n",
        "# print(\"Confidence:\", probs[torch.argmax(probs)].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "predict_rating = df_rating['reviewText'].apply(lambda x: sentiment_analysis(x[:512], model=model, tokenizer=tokenizer))\n",
        "\n",
        "predict_rating_list = predict_rating.tolist()\n",
        "predict_rating_list =  defineRating(predict_rating_list)\n",
        "print(predict_rating_list)\n",
        "\n",
        "df_rating['predict_rating'] = predict_rating_list\n",
        "print(df_rating)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sentiment_analysis(\"The color pattern and fit is what I liked the most what I liked the least is that they are not easy to clean and stains do not come out very easy or at all\", model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "\n",
        "for row in df_rating.itertuples():\n",
        "    if row.predict_rating != row.T_v:\n",
        "        i += 1\n",
        "        print(row.predict_rating)\n",
        "        print(row.T_v)\n",
        "        print(row.overall)\n",
        "        print(row.reviewText)\n",
        "        print('------------------------')\n",
        "print('count: ', i)\n",
        "\n",
        "# Tạo DataFrame mới để lưu các hàng thỏa mãn điều kiện\n",
        "outliner = pd.DataFrame(columns=['predict_rating', 'T_v', 'overall', 'reviewText'])\n",
        "# Lặp qua các hàng của DataFrame df_rating\n",
        "# Lặp qua các hàng của DataFrame df_rating\n",
        "for index, row in df_rating.iterrows():\n",
        "    if row['predict_rating'] != row['T_v']:\n",
        "        # Thêm hàng vào DataFrame outliner\n",
        "        outliner.loc[len(outliner)] = row\n",
        "\n",
        "# In ra DataFrame outliner\n",
        "print(outliner)\n",
        "\n",
        "# Lưu DataFrame outliner vào file CSV\n",
        "outliner.to_csv('/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/outliner_bert.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-04-26 01:37:15.057293: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-04-26 01:37:15.294342: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-26 01:37:16.144620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[nltk_data] Downloading package punkt to /home/triet/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/triet/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /home/triet/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-04-26 01:37:18,719 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import math\n",
        "import spacy\n",
        "import re\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from flair.data import Sentence\n",
        "from flair.nn import Classifier\n",
        "from flair.models import TextClassifier\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "\n",
        "a = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
        "brands = ['dove', 'aveda', 'silver']\n",
        "keywords = ['overall', 'summary']\n",
        "tagger = Classifier.load('pos')\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def remove_stopwords(sentence):\n",
        "    \"\"\"\n",
        "    Loại bỏ các từ dừng từ câu sử dụng NLTK.\n",
        "\n",
        "    Parameters:\n",
        "    - sentence (str): Câu cần loại bỏ từ dừng.\n",
        "\n",
        "    Returns:\n",
        "    - str: Câu sau khi loại bỏ các từ dừng.\n",
        "    \"\"\"\n",
        "    # Tách câu thành các từ\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    # Tải danh sách từ dừng từ NLTK\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Loại bỏ các từ dừng\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Ghép lại các từ thành câu mới\n",
        "    filtered_sentence = ' '.join(filtered_words)\n",
        "\n",
        "    return filtered_sentence\n",
        "\n",
        "# df_rating['reviewText'] = df_rating['reviewText'].apply(remove_stopwords)\n",
        "\n",
        "def get_sentiment_words(text):\n",
        "    # Khởi tạo SentimentIntensityAnalyzer\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Tách câu thành các từ\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Lấy danh sách các từ ngừng (stop words)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Danh sách các từ có cảm xúc\n",
        "    sentiment_words = []\n",
        "\n",
        "    # Kiểm tra từng từ trong câu\n",
        "    for word in words:\n",
        "        print(word, \" - \", sia.polarity_scores(word)['compound'])\n",
        "        # Nếu từ không phải là từ ngừng và có điểm tính cảm từ VADER lớn hơn 0\n",
        "        # if word.lower() not in stop_words and sia.polarity_scores(word)['compound'] > 0:\n",
        "        if word.lower() not in stop_words and sia.polarity_scores(word)['compound'] != 0:\n",
        "            sentiment_words.append(word)\n",
        "\n",
        "    return sentiment_words\n",
        "\n",
        "def analysis_meaning_flair(sentence):\n",
        "\n",
        "    # load model\n",
        "    tagger = Classifier.load('frame')\n",
        "\n",
        "    # make English sentence\n",
        "    sentence = Sentence(sentence)\n",
        "\n",
        "    # predict NER tags\n",
        "    tagger.predict(sentence)\n",
        "\n",
        "    # go through tokens and print predicted frame (if one is predicted)\n",
        "    for token in sentence:\n",
        "        print(token)\n",
        "\n",
        "def sentiment_analysis_flair(sentence):\n",
        "    # Kiểm tra xem câu có rỗng không\n",
        "    if not sentence.strip():\n",
        "        return 0.0\n",
        "    # Load pretrained sentiment model\n",
        "    classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "    # Create a Flair Sentence\n",
        "    flair_sentence = Sentence(sentence)\n",
        "\n",
        "    # Predict sentiment\n",
        "    classifier.predict(flair_sentence)\n",
        "    # print(flair_sentence)\n",
        "    # Get sentiment value (POSITIVE/NEGATIVE) and score\n",
        "    sentiment_value = flair_sentence.labels[0].value\n",
        "    sentiment_score = flair_sentence.labels[0].score\n",
        "    score = 0.0\n",
        "    bias = 1.0\n",
        "    for keyword in keywords:\n",
        "        if keyword.lower() in sentence.lower():\n",
        "            bias = 1.75\n",
        "    \n",
        "    if(sentiment_value == 'POSITIVE'):\n",
        "        score = sentiment_score * bias\n",
        "    else:\n",
        "        score = -sentiment_score * bias\n",
        "\n",
        "    return score\n",
        "\n",
        "#Define the text to analyze\n",
        "\n",
        "def sentiment_analysis(text, model, tokenizer):\n",
        "    # Tokenize the text\n",
        "    # print(text)\n",
        "    if not text:\n",
        "        return -1\n",
        "    text = text[:512]\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "    # Perform sentiment analysis\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the predicted probabilities for each class\n",
        "    probs = softmax(outputs.logits, dim=1).squeeze()\n",
        "    # print(probs)\n",
        "    # Convert the probabilities to human-readable labels\n",
        "    labels = [1, 2, 3, 4, 5]  # For this model, it predicts star ratings\n",
        "    \n",
        "    predicted_label = labels[torch.argmax(probs).item()]\n",
        "    # print(predicted_label)\n",
        "    return predicted_label\n",
        "\n",
        "def check_string(input_value):\n",
        "    # Kiểm tra xem giá trị có phải là chuỗi hay không\n",
        "    if isinstance(input_value, str):\n",
        "        # Nếu là chuỗi, trả về giá trị không thay đổi\n",
        "        return input_value\n",
        "    else:\n",
        "        # Nếu không phải là chuỗi, trả về chuỗi rỗng\n",
        "        return ''\n",
        "\n",
        "def seperate_sentences(text):\n",
        "    text = check_string(text)\n",
        "    doc = nlp(text)\n",
        "    # Khởi tạo một danh sách để lưu trữ các câu\n",
        "    sentences = []\n",
        "    current_sentence = \"\"\n",
        "    # Duyệt qua mỗi từ trong văn bản\n",
        "    for token in doc:\n",
        "        # Nếu từ hiện tại không phải là dấu kết thúc câu, thêm từ vào câu hiện tại\n",
        "        \n",
        "        if token.is_sent_start:\n",
        "            # Nếu có câu hiện tại, thêm câu vào danh sách câu\n",
        "            if current_sentence:\n",
        "                sentences.append(current_sentence.strip())\n",
        "            # Bắt đầu một câu mới\n",
        "            current_sentence = token.text\n",
        "        elif token.text in [\"but\", \"and\"]:\n",
        "            # print(\"that but:\", current_sentence)\n",
        "            if (brand.lower() in current_sentence.lower() for brand in brands):\n",
        "                # print(\"need continue: \", current_sentence)\n",
        "                current_sentence += \" \" + token.text\n",
        "                continue\n",
        "            elif current_sentence:\n",
        "                sentences.append(current_sentence.strip())\n",
        "                current_sentence = token.text\n",
        "        else:\n",
        "            # Thêm từ vào câu hiện tại\n",
        "            current_sentence += \" \" + token.text\n",
        "    # Thêm câu cuối cùng vào danh sách câu\n",
        "    if current_sentence:\n",
        "        sentences.append(current_sentence.strip())\n",
        "    return sentences\n",
        "\n",
        "def split_sentences_and_filter_sentiment(text):\n",
        "    # Initialize SentimentIntensityAnalyzer\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Tách đoạn văn thành các câu nhỏ\n",
        "    # sentences = nltk.sent_tokenize(text)\n",
        "    sentences = seperate_sentences(text)\n",
        "    if len(sentences) == 1 and not re.search(r'[.!?]', text):\n",
        "        sentences = [text]\n",
        "    # print(sentences)\n",
        "    # Lọc các câu không chứa bất kỳ từ miêu tả cảm xúc nào\n",
        "    filtered_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # print(\"---- sentence ------\")\n",
        "        if not sentence.strip():\n",
        "            continue\n",
        "        sentence = sentence.strip()\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        \n",
        "        # for word in words:\n",
        "        #     if(sia.polarity_scores(word)['compound'] != 0 ):\n",
        "        #         print(word, \":\", sia.polarity_scores(word)['compound'])\n",
        "\n",
        "        contains_sentiment = any(sia.polarity_scores(word)['compound'] != 0 for word in words)\n",
        "        for word in words:\n",
        "            if(word.lower() in brands):\n",
        "                contains_sentiment = False\n",
        "        # Flair\n",
        "        \n",
        "        sentence_flair = Sentence(sentence)\n",
        "        tagger.predict(sentence_flair)\n",
        "        # print(sentence_flair.tokens)\n",
        "        is_a = False\n",
        "        for token in sentence_flair.tokens:\n",
        "            for label in token.get_labels('pos'):\n",
        "                if label.value in a:\n",
        "                    # print(token, label.value)\n",
        "                    is_a = True\n",
        "                    break\n",
        "        for word in words:\n",
        "            if(word.lower() in brands):\n",
        "                contains_sentiment = False\n",
        "                is_a = False\n",
        "        #--------------\n",
        "        if contains_sentiment:\n",
        "            filtered_sentences.append(sentence)\n",
        "        elif is_a:\n",
        "            filtered_sentences.append(sentence)\n",
        "        else:\n",
        "            print(\"remove: \", sentence)\n",
        "            # print(evaluate_sentiment_flair(sentence))\n",
        "        # print(\"---- end sentence ------\")\n",
        "    # Thêm câu vào danh sách nếu không có câu nào chứa từ miêu tả cảm xúc\n",
        "    if not filtered_sentences:\n",
        "        filtered_sentences = [\"\"]\n",
        "    # print(filtered_sentences)\n",
        "    return filtered_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer for Amazon reviews\n",
        "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'  # Example model, you can choose other pre-trained models\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remove:  <3\n",
            "If they discontinued this scent , I would go postal . -0.9895524382591248\n",
            "-0.9895524382591248\n",
            "Result: -1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "predictRating = []\n",
        "text = \"If they discontinued this scent I would go postal. <3\"\n",
        "# text = remove_stopwords(text)\n",
        "\n",
        "\n",
        "sentences = split_sentences_and_filter_sentiment(text)\n",
        "total_sentiment = 0\n",
        "for sentence in sentences:\n",
        "    # sentence = remove_stopwords(sentence)\n",
        "    score = sentiment_analysis_flair(sentence)\n",
        "    print(sentence, score)\n",
        "    total_sentiment += score\n",
        "    # print(total_sentiment)\n",
        "print(total_sentiment)\n",
        "if total_sentiment > 0.0:\n",
        "    predictRating.append(1)\n",
        "    print(\"Result: 1\")\n",
        "else:\n",
        "    predictRating.append(-1)\n",
        "    print(\"Result: -1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   0%|          | 1/169781 [00:05<235:51:59,  5.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remove:  bien\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   0%|          | 4/169781 [00:12<141:29:19,  3.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remove:  What can I say ?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   0%|          | 9/169781 [00:35<261:17:57,  5.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remove:  I had Lifesong\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   0%|          | 11/169781 [01:04<484:12:14, 10.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remove:  Move on to another topic ! ! !\n",
            "remove:  It will collect dust on my shelf .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   0%|          | 12/169781 [01:21<578:49:35, 12.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remove:  It was the lyrics .\n",
            "remove:  I decided to go ahead and buy that album and found every song spoke volumes to me .\n",
            "remove:  Song # 2 :\n",
            "remove:  Every Man-\n",
            "remove:  One of the ladies from Casting Crowns sings this .\n",
            "remove:  # 11 : White Dove : This song is completely different from anything Casting Crowns has ever done .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   0%|          | 12/169781 [02:15<533:50:32, 11.32s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m         output_df\u001b[38;5;241m.\u001b[39mto_csv(output_file, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_file), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Sử dụng hàm\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mprocess_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_rating\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/output.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mprocess_and_save\u001b[0;34m(df_rating, output_file)\u001b[0m\n\u001b[1;32m     11\u001b[0m sentences \u001b[38;5;241m=\u001b[39m split_sentences_and_filter_sentiment(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviewText\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m filtered_sentences\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentences))\n\u001b[0;32m---> 13\u001b[0m total_sentiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentiment_analysis_flair(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_sentiment \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m     15\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m sentences \u001b[38;5;241m=\u001b[39m split_sentences_and_filter_sentiment(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviewText\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m filtered_sentences\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentences))\n\u001b[0;32m---> 13\u001b[0m total_sentiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43msentiment_analysis_flair\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_sentiment \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m     15\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "Cell \u001b[0;32mIn[7], line 92\u001b[0m, in \u001b[0;36msentiment_analysis_flair\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Load pretrained sentiment model\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mTextClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-sentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Create a Flair Sentence\u001b[39;00m\n\u001b[1;32m     95\u001b[0m flair_sentence \u001b[38;5;241m=\u001b[39m Sentence(sentence)\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/flair/models/text_classification_model.py:139\u001b[0m, in \u001b[0;36mTextClassifier.load\u001b[0;34m(cls, model_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, model_path: Union[\u001b[38;5;28mstr\u001b[39m, Path, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cast\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/flair/nn/model.py:978\u001b[0m, in \u001b[0;36mDefaultClassifier.load\u001b[0;34m(cls, model_path)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, model_path: Union[\u001b[38;5;28mstr\u001b[39m, Path, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefaultClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cast\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefaultClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/flair/nn/model.py:555\u001b[0m, in \u001b[0;36mClassifier.load\u001b[0;34m(cls, model_path)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, model_path: Union[\u001b[38;5;28mstr\u001b[39m, Path, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cast\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/flair/nn/model.py:179\u001b[0m, in \u001b[0;36mModel.load\u001b[0;34m(cls, model_path)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_path, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    178\u001b[0m     model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_model(\u001b[38;5;28mstr\u001b[39m(model_path))\n\u001b[0;32m--> 179\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mload_torch_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     state \u001b[38;5;241m=\u001b[39m model_path\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/flair/file_utils.py:352\u001b[0m, in \u001b[0;36mload_torch_state\u001b[0;34m(model_file)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# load_big_file is a workaround byhttps://github.com/highway11git\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# to load models on some Mac/Windows setups\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# see https://github.com/zalandoresearch/flair/issues/351\u001b[39;00m\n\u001b[1;32m    351\u001b[0m f \u001b[38;5;241m=\u001b[39m load_big_file(model_file)\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/flair/embeddings/transformer.py:1244\u001b[0m, in \u001b[0;36mTransformerEmbeddings.__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[model_type]\n\u001b[1;32m   1242\u001b[0m     config \u001b[38;5;241m=\u001b[39m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_state_dict)\n\u001b[0;32m-> 1244\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;66;03m# copy values from new embedding\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m embedding\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/flair/embeddings/document.py:63\u001b[0m, in \u001b[0;36mTransformerDocumentEmbeddings.create_from_state\u001b[0;34m(cls, **state)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_from_state\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# this parameter is fixed\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_document_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/flair/embeddings/document.py:49\u001b[0m, in \u001b[0;36mTransformerDocumentEmbeddings.__init__\u001b[0;34m(self, model, layers, layer_mean, is_token_embedding, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     32\u001b[0m     model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# set parameters with different default values\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Bidirectional transformer embeddings of words from various transformer architectures.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m        **kwargs: Arguments propagated to :meth:`flair.embeddings.transformer.TransformerEmbeddings.__init__`\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mTransformerEmbeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_token_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_token_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_document_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/flair/embeddings/transformer.py:1047\u001b[0m, in \u001b[0;36mTransformerEmbeddings.__init__\u001b[0;34m(self, model, fine_tune, layers, layer_mean, subtoken_pooling, cls_pooling, is_token_embedding, is_document_embedding, allow_long_sentences, use_context, respect_document_boundaries, context_dropout, saved_config, tokenizer_data, feature_extractor_data, name, force_max_length, needs_manual_ocr, use_context_separator, **kwargs)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoFeatureExtractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_ocr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/transformers/models/auto/feature_extraction_auto.py:341\u001b[0m, in \u001b[0;36mAutoFeatureExtractor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    339\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureExtractionMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_extractor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m feature_extractor_class \u001b[38;5;241m=\u001b[39m config_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_extractor_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    343\u001b[0m feature_extractor_auto_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:498\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.get_feature_extractor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m feature_extractor_file \u001b[38;5;241m=\u001b[39m FEATURE_EXTRACTOR_NAME\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     resolved_feature_extractor_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_extractor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1671\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 369\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:392\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:68\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     70\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "File \u001b[0;32m~/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/usr/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/usr/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "import os\n",
        "\n",
        "def process_and_save(df_rating, output_file):\n",
        "    predictRating = []\n",
        "    filtered_sentences = []\n",
        "    output_df = pd.DataFrame(columns=['reviewID', 'asin', 'predictRating', 'T_v', 'overall', 'reviewText'])\n",
        "    score = 0\n",
        "    # Đoạn mã dùng tqdm để hiển thị tiến trình %\n",
        "    for i, row in tqdm.tqdm(df_rating.iterrows(), desc=\"Processing\", total=len(df_rating)):\n",
        "        sentences = split_sentences_and_filter_sentiment(row['reviewText'])\n",
        "        filtered_sentences.append(\" \".join(sentences))\n",
        "        total_sentiment = sum(sentiment_analysis_flair(sentence) for sentence in sentences)\n",
        "        if total_sentiment > 0.0:\n",
        "            score = 1\n",
        "        else:\n",
        "            score = -1\n",
        "        predictRating.append(score)\n",
        "        # Lưu vào DataFrame và ghi vào file CSV sau mỗi 10 record\n",
        "        output_df.loc[len(output_df)] = { 'reviewID': row['reviewerID'],\n",
        "                                          'asin': row['asin'],\n",
        "                                          'predictRating': score,\n",
        "                                          'T_v': row['T_v'],\n",
        "                                          'overall': row['overall'],\n",
        "                                          'reviewText': row['reviewText']}\n",
        "        \n",
        "        if (i + 1) % 5 == 0:\n",
        "            output_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
        "            output_df = pd.DataFrame(columns=['reviewID', 'asin', 'predictRating', 'T_v', 'overall', 'reviewText'])\n",
        "    \n",
        "    # Ghi phần còn lại của DataFrame vào file CSV\n",
        "    if not output_df.empty:\n",
        "        output_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
        "\n",
        "\n",
        "# Sử dụng hàm\n",
        "process_and_save(df_rating, '/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/output.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "    predictRating T_v overall  \\\n",
            "0              -1   1     5.0   \n",
            "1              -1   1     4.0   \n",
            "2              -1   1     5.0   \n",
            "3              -1   1     4.0   \n",
            "4              -1   1     5.0   \n",
            "..            ...  ..     ...   \n",
            "362            -1   1     5.0   \n",
            "363            -1   1     5.0   \n",
            "364            -1   1     5.0   \n",
            "365            -1   1     5.0   \n",
            "366            -1   1     5.0   \n",
            "\n",
            "                                            reviewText  \n",
            "0    I received the shampoo because I was suffering...  \n",
            "1                              Good but kind of drying  \n",
            "2    It gets messy after the first few times, but c...  \n",
            "3    I've used a chemical shampoo for too long and ...  \n",
            "4    I was using this shampoo from six years and fo...  \n",
            "..                                                 ...  \n",
            "362  I've been using this product for a couple of y...  \n",
            "363                                              soapy  \n",
            "364  If they discontinued this scent I would go pos...  \n",
            "365  This would be a good conditioner to use every ...  \n",
            "366  This has helped my hair more than any other pr...  \n",
            "\n",
            "[367 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "print(predictRating)\n",
        "\n",
        "df_rating['predictRating'] = predictRating\n",
        "# Tạo DataFrame mới để lưu các hàng thỏa mãn điều kiện\n",
        "outliner = pd.DataFrame(columns=['predictRating', 'T_v', 'overall', 'reviewText'])\n",
        "# Lặp qua các hàng của DataFrame df_rating\n",
        "# Lặp qua các hàng của DataFrame df_rating\n",
        "for index, row in df_rating.iterrows():\n",
        "    if row['predictRating'] != row['T_v']:\n",
        "        # Thêm hàng vào DataFrame outliner\n",
        "        outliner.loc[len(outliner)] = row\n",
        "\n",
        "# In ra DataFrame outliner\n",
        "print(outliner)\n",
        "\n",
        "# Lưu DataFrame outliner vào file CSV\n",
        "outliner.to_csv('/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/outliner.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        id  T_v      reviewerID        asin  overall  \\\n",
            "0        0    1  A3CIUOJXQ5VDQ2  B0000530HU      5.0   \n",
            "1        1    1  A3H7T87S984REU  B0000530HU      5.0   \n",
            "2        2   -1  A3J034YH7UG4KT  B0000530HU      1.0   \n",
            "3        3    1  A2UEO5XR3598GI  B0000530HU      5.0   \n",
            "4        4    1  A3SFRT223XXWF7  B00006L9LC      5.0   \n",
            "...    ...  ...             ...         ...      ...   \n",
            "5259  5259    1   AUX122XW8ONG6  B01DLR9IDI      5.0   \n",
            "5260  5260    1   AUX122XW8ONG6  B01DLR9IDI      5.0   \n",
            "5261  5261    1   AUX122XW8ONG6  B01DLR9IDI      5.0   \n",
            "5262  5262    1   AUX122XW8ONG6  B01DLR9IDI      5.0   \n",
            "5263  5263    1   AUX122XW8ONG6  B01DLR9IDI      5.0   \n",
            "\n",
            "                                             reviewText  predictRating  \n",
            "0                                     Reasonably priced              1  \n",
            "1     Like the oder and the feel when I put it on my...              1  \n",
            "2     I bought this to smell nice after I shave . Wh...             -1  \n",
            "3     I am an Aqua Velva Man and absolutely love thi...              1  \n",
            "4     If you ever want to feel pampered by a shampoo...              1  \n",
            "...                                                 ...            ...  \n",
            "5259  I have genetic undereye darkness . I ve accept...              1  \n",
            "5260                   I absolutely love this eye gel .              1  \n",
            "5261  The eye gel is easy to apply and I use it morn...              1  \n",
            "5262                    Ok this eye gel is good stuff .              1  \n",
            "5263  This is the first eye gel / cream that actuall...              1  \n",
            "\n",
            "[5264 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "df_rating['reviewText'] = filtered_sentences\n",
        "print(df_rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset:  5264\n",
            "Dataset remover duplication:  5264\n",
            "Dataset removed outliner:  4897\n",
            "Filtered dataset:  4897\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# for index, row in df_rating.iterrows():\n",
        "#     if row['predictRating'] != row['T_v']:\n",
        "#         df_rating.drop(index, inplace=True)\n",
        "# print(\"Dataset removed outliner: \", len(df_rating))\n",
        "\n",
        "#Load json data to dataframe\n",
        "all_beauty_data = pd.read_json('/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/data/raw/All_Beauty_5.json', lines=True)\n",
        "print(\"Dataset: \", len(all_beauty_data))\n",
        "# Loại bỏ các dòng có reviewText trùng lặp\n",
        "# all_beauty_data = remove_duplicate_rows(all_beauty_data)\n",
        "print(\"Dataset remover duplication: \", len(all_beauty_data))\n",
        "all_beauty_data['reviewText'] = filtered_sentences\n",
        "all_beauty_data['T_v'] = T_v\n",
        "all_beauty_data['predictRating'] = predictRating\n",
        "# Kiểm tra các bản ghi trong all_beauty_data mà không có trong df_rating\n",
        "for index, row in all_beauty_data.iterrows():\n",
        "    if row['predictRating'] != row['T_v']:\n",
        "        all_beauty_data.drop(index, inplace=True)\n",
        "print(\"Dataset removed outliner: \", len(all_beauty_data))\n",
        "\n",
        "all_beauty_data.drop(columns=['T_v', 'predictRating'], inplace=True)\n",
        "\n",
        "print(\"Filtered dataset: \", len(all_beauty_data))\n",
        "all_beauty_data.to_json('/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/data/raw/All_Beauty_Filtered.json', orient='records', lines=True)\n",
        "\n",
        "all_beauty_data_filtered = pd.read_json('/home/triet/ComputerScience_K32/Recommendation_System/DeepCGSR_triet/data/raw/All_Beauty_Filtered.json', lines=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal sentences: I received the shampoo because I was suffering from dandruff and I was like ok lets try it and I couldn't believe the result from only just one wash. it cleared out the dandruff without drying my hair out. My hair felt smooth and Fresh after just the first wash. I tried aveda spray before but it was useless and waste of money\n",
            "Filtered sentences: This is the first eye gel/cream that actually does what it said it was gonna do. \n",
            "I received the shampoo because I was suffering from dandruff and I was like ok lets try it and I couldn't believe the result from only just one wash. it cleared out the dandruff without drying my hair out. My hair felt smooth and Fresh after just the first wash. I tried aveda spray before but it was useless and waste of money\n",
            "0\n",
            "-1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': '1 star', 'score': 0.7068440914154053}]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normal_sentence = \"I received the shampoo because I was suffering from dandruff and I was like ok lets try it and I couldn't believe the result from only just one wash. it cleared out the dandruff without drying my hair out. My hair felt smooth and Fresh after just the first wash. I tried aveda spray before but it was useless and waste of money\"\n",
        "# filtered_sentences = remove_sentences(normal_sentence)\n",
        "print(\"Normal sentences:\", normal_sentence)\n",
        "print(\"Filtered sentences:\", filtered_sentences)\n",
        "print(sentiment_analysis(normal_sentence, model, tokenizer))\n",
        "# print(sentiment_analysis(filtered_sentences, model, tokenizer))\n",
        "# for sentence in normal_sentence.split(\".\"):\n",
        "#     print(sentiment_analysis(sentence, model, tokenizer))\n",
        "    # print(evaluate_sentiment_flair(sentence))\n",
        "# print(evaluate_sentiment_flair(normal_sentence))\n",
        "# print(evaluate_sentiment_flair(normal_sentence))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
